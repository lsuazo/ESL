\documentclass[a4paper]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb}
\usepackage{xcolor}

\def\reals{\mathbb{R}}

\title{Elements of Statistical Learning}
\author{book by Hastie, Tibshirani, and Friedmann}
\date{}

\begin{document}
\maketitle
\setcounter{chapter}{1}
\chapter{Overview of Supervised Learning}
\section{Notation}
\begin{itemize}
  \item Input variable typically denoted by $X$. 
  \item if $X$ is vector, its components accessed by subscript $X_j$
  \item upper case refers to generic/abstract variable. Observed values are written with lowercase: i.e. ith observed value is $x_i$ (which again can be scalar or vector)
  \item Matrices represented by bold upper case $\bf X$
    \subitem example: a set of N input p-vectors $x_i$ where $i=1..N$, is represented as the $N\times p$ matrix $X$
  \item In general vectors are not bolded, except when they have $N$ components. This distinguishes a p-vector of inputs $x_i$ for the ith observation from the $N$-vector $\bf x_j$ consisting of all observations of variable $X_j$.
  \item All vectors are assumed to be column vectors. Hence the ith row of $\bf X$ is $x_i^T$.
\end{itemize}

\section{Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors}
\subsection{Linear Model}
\begin{itemize}
  \item Given a vector of inputs $X^T = (X_1, \cdots, X_p)$, we predict the output $Y$ via
    $$ \hat{Y}  = \beta_0 + \sum_{j=1}^p X_j \hat{\beta}_j $$
  \item often convenient to include the {\bf bias} (aka intercept) term $\beta_0$  into $X$ by including a constant variable $1$ in $X$. Then letting $\hat{\beta}$ be the vector of coefficients including the bias, we can write the linear model in vector form as the inner product
    $$ \hat{Y} = X^T\hat{\beta} $$
  \item in general, $\hat{Y}$ could be a k-vector (i.e. the output is not scalar valued), in which case $\hat{\beta}$ would be a $p\times K$ matrix of coefficients.
  \item most popular approach to fit the linear model is the method of {\bf least squares}: Pick the coefficients $\hat{\beta}$ to minimize the residual sum of squares:
    $$ RSS(\beta) := \sum_{i=1}^n (y_i - x_i^T\beta)^2$$
    in matrix notation
    $$ RSS(\beta) = ({\bf y} - {\bf X}\beta)^T({\bf y} - {\bf X}\beta) $$
    \subitem for nonsingular $X^TX$ solution is
    $$ \hat{\beta} = \left(X^TX\right)^{-1} X^TY $$
\end{itemize}

\section{Nearest-Neighbor Methods}
\begin{itemize}
  \item The k-nearest neighbor fit for $\hat{Y}$ is defined as follows:
    $$ \hat{Y} = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i $$
    where $N_k(x)$ is the neighborhood of $x$ defined by the k closest points $x_i$ in the training sample.
    \subitem requires metric to define closest - typically euclidean
    \subitem No training required! That is no parameters are fit.
  \item while k-NN appears to have a single parameter $k$, in truth it has $N/k$ effective parameters, where $N$ is data size.
    \subitem this is generally much larger than linear model parameters and thus requires much more data.
    \subitem heuristic: if you have nonoverlapping clusters ok $k$ points, then you have $N/k$ such nhoods, and thus need $N/K$ parms (the means) to describe result/fit.
  \item cannot use RSS on training to pick $k$, because it would always pick $k=1$, which leads to zero training error.
\end{itemize}

\section{Statistical Decision Theory}
\begin{itemize}
  \item let $X \in \reals^p$ be real valued random input vector, $Y \in \reals$ be real valued random ouput, with joint distribution $Pr(X,Y)$
  \item we seek function $f(X):\reals^p \to \reals$ for prediting $Y$ value
  \item for this we need a {\bf loss function} $L(Y, f(X))$ for penalizing errors in prediction
  \item most common loss function is: {\bf squared error loss} 
    $$ L(Y, f(X)) = (Y - f(X))^2 $$
  \item criterion for choosing f: minimize the expected prediction error (EPE)
    $$ EPE(f) = E(Y-f(X))^2 = \int [y - f(x)]^2 Pr(dx,dy) $$
  \item He presents some short argument that via conditioning on X and pointwise minimization, you can arrive at the solution
    $$ f(x) = E(Y | X = x), $$
    that is, the \emph{conditional expecation}, aka the {\bf regression function}.
  \item {\color{gray} I am interested in how to rigorously solve this - perhaps a constrained variational calculus problem?}
  \item KNN is an approximation to this, where instead of relying only on observations at $x$ exactly, a neighborhood of $x$ is use to obtain the expected value of $y$.
  \item for linear regression - we propose ansatz $f(x) \sim x^T \beta$ -  so don't search over all functions
    \subitem this is a model based approach
    \subitem theoretical solutions is:
    $$ \beta = \left[ E(XX^T)\right]^{-1} E(XY) $$
  \item Using L1 instead of L2 as loss function leads to $ \hat{f}(x) = median(Y| X = x)$
  \item For categorical output, loss function is matrix $K\times K$ matrix $L$, where $K$ is number of categories. Zero in the diagonals, and nonnegative elsewhere
    \subitem typical is {\bf zero-one loss function} - where all off diagonals are $1$.
  \item The $EPE = E[L(G, \hat{G}(x)]$ where expectation is taken over $Pr(G, x)$
    \item using same conditioning argumetn and pointwise minimization, and zero one loss, leads to solution known as the {\bf Bayes classifier}
      $$ \hat{G} = g_k \quad \mbox{ if } \quad Pr(g_k | X = x) = max_{g \in G} Pr(g | X = x) $$
      that is, classify as the most probable class, using discrete conditional distribution $Pr(G|X)$.
      \subitem Error rate of the bayes classifier is often called the {\bf Bayes rate}.
\end{itemize}

\section{Local Methods in High Dimension}
\begin{itemize}
  \item Fitting/prediciton methods which rely on local approximations (like KNN), struggle as dimensions get high - {\bf the curse of dimentionality}
  \item example: consider p-dimensional unit hypercube with uniformly distributed inputs. If we place a sub hypercube about the origin, such that a fraction $r$ of the data is contained there,
    the linear dimension of this hypercube must be $r^{1/p}$. For 10 dimensions, the expected edge length for $r=0.01$ is $0.63$.
    \subitem so to capture one percent of the data, you must go $\%63$ of the distance avaialble in each dimension! That is not local.
  \item another manifestation of this curse is that the sampling density is proportional to $N^{1/}$ where $N$ is sample size and $p$ is dimension. So if in 1-D $N=100$ represents a dense sample size, the equivalent density in 10 D is $100^{10}$. 
    \subitem so in high D, all feasable samples are sparse.
  \item {\bf Bias-variance tradeoff} - nice little ``proof'' - setup: fix a point in domain $x_0$, get a large \emph{set of training samples} $\tau$ - study the expected error in prediction at $x_0$ that comes from the sampling.
    \subitem notation: $\hat{y}_0$ is prediction using some model. $y_0$ is true value.
    \begin{eqnarray}
      MSE(x_0) &=& E_\tau[(y_0 - \hat{y}_0)^2]\\
      	&=& E_\tau[(y_0 - E_\tau[\hat{y}_0] + E_\tau[\hat{y}_0] - \hat{y}_0)^2 ] \\
	&=& E_\tau[(y_0 - E_\tau[\hat{y}_0])^2] + 2E_\tau[(y_0 - E_\tau[\hat{y}_0])(E_\tau[\hat{y}_0] - \hat{y}_0)] +   {\color{blue} E_\tau[(\hat{y}_0 - E_\tau[\hat{y}_0])^2] }\\
	&=&  E_\tau[y_0^2] - 2E_\tau[y_0E\tau[\hat{y}_0]] + E_\tau[E_\tau[\hat{y}_0]^2] + 2E\tau[y_0E_\tau[\hat{y}_0]] - 2E_\tau[y_0\hat{y}_0] - 2E_\tau[E_\tau[\hat{y}_0]^2] + 2E_\tau[E_\tau[\hat{y}_0]\hat{y}_0] \nonumber
\\ 	&& + {\color{blue} Var_\tau(\hat{y}_0)} \\
&=& {\color{orange} y_0^2 -2y_0E_\tau[\hat{y}_0] + E_\tau[\hat{y}_0]^2} + {\color{red} 2y_0E_\tau[\hat{y}_0] - 2y_0E_\tau[\hat{y}_0] - 2E_\tau[\hat{y}_0]^2 + 2E_\tau[\hat{y}_0]^2} + {\color{blue} Var_\tau(\hat{y}_0)} \\
	&=& {\color{orange} (y_0 - E_\tau[\hat{y}_0])^2} + {\color{blue} Var_\tau(\hat{y}_0)} \\ 
	&=& {\color{orange} Bias_\tau(\hat{y}_0)^2} + {\color{blue} Var_\tau(\hat{y}_0)}
    \end{eqnarray}
    \subitem I think must consider one sample $S \in \tau$, to get $\hat{y}_0$, and then consider all samples $\tau$ to talk about $E_\tau[\hat{y}_0]$.
\end{itemize}

\end{document}
