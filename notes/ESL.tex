\documentclass[a4paper]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb}
\usepackage{xcolor}


\newcommand{\brown}{\color{brown}}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\gray}{\color{gray}}
\newcommand{\<}{\textless}
\renewcommand{\>}{\textgreater}

\def\reals{\mathbb{R}}

\title{Elements of Statistical Learning}
\author{book by Hastie, Tibshirani, and Friedmann}
\date{}

\begin{document}
\maketitle
\setcounter{chapter}{1}
\chapter{Overview of Supervised Learning}
\section{Notation}
\begin{itemize}
  \item Input variable typically denoted by $X$. 
  \item if $X$ is vector, its components accessed by subscript $X_j$
  \item upper case refers to generic/abstract variable. Observed values are written with lowercase: i.e. ith observed value is $x_i$ (which again can be scalar or vector)
  \item Matrices represented by bold upper case $\bf X$
    \subitem example: a set of N input p-vectors $x_i$ where $i=1..N$, is represented as the $N\times p$ matrix $X$
  \item In general vectors are not bolded, except when they have $N$ components. This distinguishes a p-vector of inputs $x_i$ for the ith observation from the $N$-vector $\bf x_j$ consisting of all observations of variable $X_j$.
  \item All vectors are assumed to be column vectors. Hence the ith row of $\bf X$ is $x_i^T$.
\end{itemize}

\section{Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors}
\subsection{Linear Model}
\begin{itemize}
  \item Given a vector of inputs $X^T = (X_1, \cdots, X_p)$, we predict the output $Y$ via
    $$ \hat{Y}  = \beta_0 + \sum_{j=1}^p X_j \hat{\beta}_j $$
  \item often convenient to include the {\bf bias} (aka intercept) term $\beta_0$  into $X$ by including a constant variable $1$ in $X$. Then letting $\hat{\beta}$ be the vector of coefficients including the bias, we can write the linear model in vector form as the inner product
    $$ \hat{Y} = X^T\hat{\beta} $$
  \item in general, $\hat{Y}$ could be a k-vector (i.e. the output is not scalar valued), in which case $\hat{\beta}$ would be a $p\times K$ matrix of coefficients.
  \item most popular approach to fit the linear model is the method of {\bf least squares}: Pick the coefficients $\hat{\beta}$ to minimize the residual sum of squares:
    $$ RSS(\beta) := \sum_{i=1}^n (y_i - x_i^T\beta)^2$$
    in matrix notation
    $$ RSS(\beta) = ({\bf y} - {\bf X}\beta)^T({\bf y} - {\bf X}\beta) $$
    \subitem for nonsingular $X^TX$ solution is
    $$ \hat{\beta} = \left(X^TX\right)^{-1} X^TY $$
\end{itemize}

\section{Nearest-Neighbor Methods}
\begin{itemize}
  \item The k-nearest neighbor fit for $\hat{Y}$ is defined as follows:
    $$ \hat{Y} = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i $$
    where $N_k(x)$ is the neighborhood of $x$ defined by the k closest points $x_i$ in the training sample.
    \subitem requires metric to define closest - typically euclidean
    \subitem No training required! That is no parameters are fit.
  \item while k-NN appears to have a single parameter $k$, in truth it has $N/k$ effective parameters, where $N$ is data size.
    \subitem this is generally much larger than linear model parameters and thus requires much more data.
    \subitem heuristic: if you have nonoverlapping clusters ok $k$ points, then you have $N/k$ such nhoods, and thus need $N/K$ parms (the means) to describe result/fit.
  \item cannot use RSS on training to pick $k$, because it would always pick $k=1$, which leads to zero training error.
\end{itemize}

\section{Statistical Decision Theory}
\begin{itemize}
  \item let $X \in \reals^p$ be real valued random input vector, $Y \in \reals$ be real valued random ouput, with joint distribution $Pr(X,Y)$
  \item we seek function $f(X):\reals^p \to \reals$ for prediting $Y$ value
  \item for this we need a {\bf loss function} $L(Y, f(X))$ for penalizing errors in prediction
  \item most common loss function is: {\bf squared error loss} 
    $$ L(Y, f(X)) = (Y - f(X))^2 $$
  \item criterion for choosing f: minimize the expected prediction error (EPE)
    $$ EPE(f) = E(Y-f(X))^2 = \int [y - f(x)]^2 Pr(dx,dy) $$
  \item He presents some short argument that via conditioning on X and pointwise minimization, you can arrive at the solution
    $$ f(x) = E(Y | X = x), $$
    that is, the \emph{conditional expecation}, aka the {\bf regression function}.
  \item {\color{gray} I am interested in how to rigorously solve this - perhaps a constrained variational calculus problem?}
  \item KNN is an approximation to this, where instead of relying only on observations at $x$ exactly, a neighborhood of $x$ is use to obtain the expected value of $y$.
  \item for linear regression - we propose ansatz $f(x) \sim x^T \beta$ -  so don't search over all functions
    \subitem this is a model based approach
    \subitem theoretical solutions is:
    $$ \beta = \left[ E(XX^T)\right]^{-1} E(XY) $$
  \item Using L1 instead of L2 as loss function leads to $ \hat{f}(x) = median(Y| X = x)$
  \item For categorical output, loss function is matrix $K\times K$ matrix $L$, where $K$ is number of categories. Zero in the diagonals, and nonnegative elsewhere
    \subitem typical is {\bf zero-one loss function} - where all off diagonals are $1$.
  \item The $EPE = E[L(G, \hat{G}(x)]$ where expectation is taken over $Pr(G, x)$
    \item using same conditioning argumetn and pointwise minimization, and zero one loss, leads to solution known as the {\bf Bayes classifier}
      $$ \hat{G} = g_k \quad \mbox{ if } \quad Pr(g_k | X = x) = max_{g \in G} Pr(g | X = x) $$
      that is, classify as the most probable class, using discrete conditional distribution $Pr(G|X)$.
      \subitem Error rate of the bayes classifier is often called the {\bf Bayes rate}.
\end{itemize}

\section{Local Methods in High Dimension}
\begin{itemize}
  \item Fitting/prediciton methods which rely on local approximations (like KNN), struggle as dimensions get high - {\bf the curse of dimentionality}
  \item example: consider p-dimensional unit hypercube with uniformly distributed inputs. If we place a sub hypercube about the origin, such that a fraction $r$ of the data is contained there,
    the linear dimension of this hypercube must be $r^{1/p}$. For 10 dimensions, the expected edge length for $r=0.01$ is $0.63$.
    \subitem so to capture one percent of the data, you must go $\%63$ of the distance avaialble in each dimension! That is not local.
  \item another manifestation of this curse is that the sampling density is proportional to $N^{1/}$ where $N$ is sample size and $p$ is dimension. So if in 1-D $N=100$ represents a dense sample size, the equivalent density in 10 D is $100^{10}$. 
    \subitem so in high D, all feasable samples are sparse.
  \item {\bf Bias-variance tradeoff} - nice little ``proof'' - setup: fix a point in domain $x_0$, get a large \emph{set of training samples} $\tau$ - study the expected error in prediction at $x_0$ that comes from the sampling.
    \subitem notation: $\hat{y}_0$ is prediction using some model. $y_0$ is true value.
    \begin{eqnarray}
      MSE(x_0) &=& E_\tau[(y_0 - \hat{y}_0)^2]\\
      	&=& E_\tau[(y_0 - E_\tau[\hat{y}_0] + E_\tau[\hat{y}_0] - \hat{y}_0)^2 ] \\
	&=& {\color{orange} E_\tau[(y_0 - E_\tau[\hat{y}_0])^2]} + 2E_\tau[(y_0 - E_\tau[\hat{y}_0])(E_\tau[\hat{y}_0] - \hat{y}_0)] +   {\color{blue} E_\tau[(\hat{y}_0 - E_\tau[\hat{y}_0])^2] }
      \end{eqnarray}
      The term in blue is the variance of the prediction.
      The term in orange: $E_\tau[(y_0 - E_\tau[\hat{y}_0])^2] = (y_0 - E_\tau[\hat{y}_0])^2$ which is just the Bias of prediction squared.
      The middle term (in black) is zero (can factor out first piece, and distribute expetation over subtraction). So
      \begin{equation}
      MSE(x_0) =  {\color{orange} Bias_\tau(\hat{y}_0)^2} + {\color{blue} Var_\tau(\hat{y}_0)}
    	\end{equation}
	This relationship ends up being very generic. More on this at the end of the chapter.
\end{itemize}


\section{Statistical Models, Supervised Learning and Functional Approximation}
\begin{itemize}
  \item Summarize overarching framework
  \item have varaibles $(X,Y)$ which we observer repeatedly
    \subitem there is some joint probability distribution for their observations $Pr(X,Y)$
    \subitem {\gray Platonically speaking}
  \item our goal is \emph{prediction}: want to find a function $f$ such that $f(X)$ is a ``reasonable'' prediction for $Y$
  \item to define reasonable, need a loss function.
  \item Sum of squared errors loss leads to optimal $f$ being $f(x) = E(Y | X=x)$ - called the {\bf regression function}.
    \subitem to get said expectation we need the conditional probability function, which we never have
    \subitem so we typically approximate the regression funcion, with something like Nearest Neighbors algorithm, or some linear model
  \item More generally, we seek to approximate conditional probability $Pr(Y | X)$
  \item one common model: {\bf additive error model}
    \subitem assumes:
    $$ Y = f(X) + \epsilon$$
    where $f$ is the regression function, $\epsilon$ is a random error independent of $X$ with some distribution such $E(\epsilon) = 0$
    \subitem {\red in what sense is error dependent or independent of $Y$?} - clearly for a given $x$, error has perfect correlation to $Y$ - excess $y$ is the error. But what about in general? Is that even a sensible question to ask?
    \subitem {\gray McElreath would say this is terrible way to think about it - better to just say something like $ Y \sim N(\mu_x, \sigma)$ and $\mu_x = f(x)$ - this generalized better to non additive models. The additive nature in this case evident from normal distro.}
  \item MAIN POINT: This is a specific claim about the conditional probability distribution $Pr(Y | X)$, namely that $Y$ is distributed like $\epsilon$ plus a value determined by $X$.
    \subitem moreover, note that $X$ only comes in through the conditional mean $f(x)$, and it does not come into the variance of $Y$ (though that can and is often relaxed)
  \item For quantitative responses, this is typically not the assumption, but rather that of some bernoulli process (for binary var) with $p$ of outcomes determined by $X$, that is $p(X)$. This binds both the conditional expectation and the variance to $x$.
    \subitem {\gray McElreath would say $Y \sim Bern(p_x)$ and $p_x = P(X)$.}
  \item His claim is that there are two main ways to think of this endeavor to find a good approximation for $f$
    \begin{itemize}
      \item Supervised learning - there is some algorithm that can take an input $x_i$ and map it to an output $\hat{f}(x_i)$, which can also adjust $\hat{f}$ based on the difference between predicted value and observed $y_i$. This algorithm should produce a map that can be used for predictions.
      \item Function Approximation - ${x_i, y_i}$ are viewed as points in a (p+1)-dimensional Euclidean space. The idea is that the data satisfies some relationship $ y_i = f(x_i) + \epsilon_i$, and goal is to obtain a useful approximation to $f$ thatn is valid for all points in some region.
	\subitem This paradigm encourages mathematical concepts of geometry and probability, so they prefer it.
    \end{itemize}
  \item often the approximations are restricted to some parameterized fammily of functions, and the challenge is to find the best parameter
    \subitem ex: linear model - $f(x) = x^T\beta$ (params are $\beta$), or more generally a {\bf linear basis expansion}
    $$f_\theta(x) = \sum_{k=1}^{K} h_k(x)\theta_k $$
    where $h_k$ are a suitable set of functions or transformatiosn of the input vector.
  \item often fit by minimizing the residual sum-of-squares (this is just least squares error).
  \item A more general criteria is {\bf maximum likelihood estimation} aka MLE	
    \begin{itemize}
      \item given random sample $y_i, i=1\cdots N$ from a density $Pr_\theta(y)$,
      \item log-probability of data is 
	$$ L(\theta) = \sum_i log(Pr_\theta(y_i)) $$
      \item the pricniple of MLE says most reasonable $\theta$ is that which maximizes $L(\theta)$
      \item Least squares with additive error model $Y = f_\theta(x) + \epsilon$ with $\epsilon \sim N(0, \sigma^2)$ is equivalent to MLE using conditional likelihood $Pr(Y | X) = N(f_\theta(X), \sigma^2)$
      \item Another example: Assume multinomial qualitative output $G$, with regression function $Pr(G|X)$. Suppose we have model $Pr(G = g_k | X = x) = p_{k,\theta}(x)$, then the loglikehood is
	$$ L(\theta) = \sum_{i=1}^N \log p_{g_i, \theta}(x_i) $$
	which is also referred to as the {\bf cross-entropy}
    \end{itemize}
  \item {\gray From entropy, $\int_x p_x log(p_x)$ to cross entropy $\int_x u_x log p_x$, then to observed cross entropy (the $\int_x u_x$ become the sum ovber observed values) $\sum_i log p(x_i)$}
\end{itemize}	

\end{document}
