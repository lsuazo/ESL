\documentclass[a4paper]{report}
\usepackage[margin=0.5in]{geometry}
\usepackage{amssymb}
\usepackage{xcolor}


\newcommand{\brown}{\color{brown}}
\newcommand{\red}{\color{red}}
\newcommand{\blue}{\color{blue}}
\newcommand{\gray}{\color{gray}}
\newcommand{\<}{\textless}
\newcommand{\yh}{\hat{y}}
\newcommand{\bh}{\hat{\beta}}
\newcommand{\yb}{\bar{y}}
\newcommand{\bX}{{\bf{X}}}
\newcommand{\bx}{{\bf{x}}}
\newcommand{\by}{{\bf{y}}}
\renewcommand{\>}{\textgreater}

\def\reals{\mathbb{R}}

\title{Elements of Statistical Learning}
\author{book by Hastie, Tibshirani, and Friedmann}
\date{}

\begin{document}
\maketitle
\chapter{LRS - Important stuff}
\section{Questions}
\begin{itemize}
  \item When he writes $f$ in ch2, is he referring specifically to the regression function $f(x) = E(Y|X=x)$ - or just any function? Is there a difference here, since the regression function, without any details on the joint distribution function is a just any function.
  \item What is the real content behind the ``additive error assumption``? 
    \begin{itemize}
      \item I.e., when he says, assume:
	    $$ Y = f(x) + \epsilon $$
	    where $E(\epsilon) = 0$ and $Var(\epsilon) = \sigma^2$.
	    \item Is f here the \emph{regression function} $f(x) = E(Y | X = x)$ by assumption? (it seems to be a consequence at the very least)
	    \item What is the Expectation in $E(\epsilon) = 0$ taken over?
	    \item What else is hidden in this assumtpion? how general or restrictive is it?
	    \item How is the joint probability distribution $Pr(X,Y)$ information encoded in here? And should $\epsilon$ be considered as another random variable.
	\end{itemize}
  \item I like the idea of many variables $x_1, \cdots x_v$ with independent prob densities, and $Y$ exactly determined by the vars. But then not measuring many variables, and hiding all their effects on $Y$ in a single $\epsilon$ r.v. -  so really we have $(X,\epsilon)$ as random variables.
    \subitem develop this further\ldots.
  \item excercise 2.6 - what does \emph{reduced weighted least squares} mean?
\end{itemize}

\section{TODO}
\begin{itemize}
  \item  sovle problem 2.5 (after reading first few sections of chapter 3)
\end{itemize}


\setcounter{chapter}{1}
\chapter{Overview of Supervised Learning}

\setcounter{section}{-1}
\section{LRS Summary of Main Ideas}
\begin{itemize}
  \item have observables $(X,Y)$
  \item want to predict $Y$ based on observations of $X$
  \item given an observation of $x$, best predictor for $Y$ (where best is defined via squared error loss function) is $f(x) = E(Y | X = x)$
  \item I assume this conditional expectation is defined using generally unkown joint probability distribution $Pr(X,Y)$
  \item ultimately we are trying to find a useful approximation for $f$
  \item The class of nearest neighbor methods can be viewed as a direct approximation to this conidtional expectation
    \subitem suffers from the curse of dimensionality
  \item talks about linear regression, a different class of model
    \subitem no curse of dimensionality - but potentially high bias
  \item generally most pairs $(X,Y)$ will not have a deterministic relationship like $Y = f(X)$
    \subitem other unmeasured variables that also contribute to $Y$, including measurement error.
  \item often assume $Y = f(X) + \epsilon$ where $E(\epsilon) = 0$, there errors are independent of $X$, and are indentically distributed
    \subitem I am still uncertain as to its significance and import of this assumption {\gray
    \subitem I guess generally all you have for sure is a conditioanl distribution $P(Y | X = x)$
    $$ f_{Y|X}(y|x) = \frac{f_{X,Y}(x,y)}{f_X(x)} $$
    where $f_X$ is the marginal density 
  $$f_X(x) = \int_y f(x,y)dy $$ }
  \item discusses two paradigms for obtaining said approximation $\hat{f}$ to $f$, given some data; the algorithmic gray box view, vs the geometric functional approximation view
    \subitem I believe it is fruitfull to label the approximation to $f(x)$ and $\hat{f}(x;D)$; It acknowledges that you need both a procedure to get the approximation, and some data.
  \item He then talks about the {\bf complexity} of models and the bias variance tradeof.
    \subitem important point in all these proofs is to consider a single prediction point $x_0$ and a set $\tau$ of different (all possible?) training data sets $D$.
    \subitem in the additive error model this encompasses both the observed $x$'s and the not directly observed $\epsilon$, which he typically just uses $Y = f + \epsilon$ assumption to separate the sources of error.
    \subitem I this part should be done more clearly.
\end{itemize}

\section{Notation}
\begin{itemize}
  \item Input variable typically denoted by $X$. 
  \item if $X$ is vector, its components accessed by subscript $X_j$
  \item upper case refers to generic/abstract variable. Observed values are written with lowercase: i.e. ith observed value is $x_i$ (which again can be scalar or vector)
  \item Matrices represented by bold upper case $\bf X$
    \subitem example: a set of N input p-vectors $x_i$ where $i=1..N$, is represented as the $N\times p$ matrix $X$
  \item In general vectors are not bolded, except when they have $N$ components. This distinguishes a p-vector of inputs $x_i$ for the ith observation from the $N$-vector $\bf x_j$ consisting of all observations of variable $X_j$.
  \item All vectors are assumed to be column vectors. Hence the ith row of $\bf X$ is $x_i^T$.
\end{itemize}

\section{Two Simple Approaches to Prediction: Least Squares and Nearest Neighbors}
\subsection{Linear Model}
\begin{itemize}
  \item Given a vector of inputs $X^T = (X_1, \cdots, X_p)$, we predict the output $Y$ via
    $$ \hat{Y}  = \beta_0 + \sum_{j=1}^p X_j \hat{\beta}_j $$
  \item often convenient to include the {\bf bias} (aka intercept) term $\beta_0$  into $X$ by including a constant variable $1$ in $X$. Then letting $\hat{\beta}$ be the vector of coefficients including the bias, we can write the linear model in vector form as the inner product
    $$ \hat{Y} = X^T\hat{\beta} $$
  \item in general, $\hat{Y}$ could be a k-vector (i.e. the output is not scalar valued), in which case $\hat{\beta}$ would be a $p\times K$ matrix of coefficients.
  \item most popular approach to fit the linear model is the method of {\bf least squares}: Pick the coefficients $\hat{\beta}$ to minimize the residual sum of squares:
    $$ RSS(\beta) := \sum_{i=1}^n (y_i - x_i^T\beta)^2$$
    in matrix notation
    $$ RSS(\beta) = ({\bf y} - {\bf X}\beta)^T({\bf y} - {\bf X}\beta) $$
    \subitem for nonsingular $X^TX$ solution is
    $$ \hat{\beta} = \left(X^TX\right)^{-1} X^TY $$
\end{itemize}

\section{Nearest-Neighbor Methods}
\begin{itemize}
  \item The k-nearest neighbor fit for $\hat{Y}$ is defined as follows:
    $$ \hat{Y} = \frac{1}{k} \sum_{x_i \in N_k(x)} y_i $$
    where $N_k(x)$ is the neighborhood of $x$ defined by the k closest points $x_i$ in the training sample.
    \subitem requires metric to define closest - typically euclidean
    \subitem No training required! That is no parameters are fit.
  \item while k-NN appears to have a single parameter $k$, in truth it has $N/k$ effective parameters, where $N$ is data size.
    \subitem this is generally much larger than linear model parameters and thus requires much more data.
    \subitem heuristic: if you have nonoverlapping clusters ok $k$ points, then you have $N/k$ such nhoods, and thus need $N/K$ parms (the means) to describe result/fit.
  \item cannot use RSS on training to pick $k$, because it would always pick $k=1$, which leads to zero training error.
\end{itemize}

\section{Statistical Decision Theory}
\begin{itemize}
  \item let $X \in \reals^p$ be real valued random input vector, $Y \in \reals$ be real valued random ouput, with joint distribution $Pr(X,Y)$
  \item we seek function $f(X):\reals^p \to \reals$ for prediting $Y$ value
  \item for this we need a {\bf loss function} $L(Y, f(X))$ for penalizing errors in prediction
  \item most common loss function is: {\bf squared error loss} 
    $$ L(Y, f(X)) = (Y - f(X))^2 $$
  \item criterion for choosing f: minimize the expected prediction error (EPE)
    $$ EPE(f) = E(Y-f(X))^2 = \int [y - f(x)]^2 Pr(dx,dy) $$
  \item He presents some short argument that via conditioning on X and pointwise minimization, you can arrive at the solution
    $$ f(x) = E(Y | X = x), $$
    that is, the \emph{conditional expecation}, aka the {\bf regression function}.
  \item {\color{gray} I am interested in how to rigorously solve this - perhaps a constrained variational calculus problem?}
  \item KNN is an approximation to this, where instead of relying only on observations at $x$ exactly, a neighborhood of $x$ is use to obtain the expected value of $y$.
  \item for linear regression - we propose ansatz $f(x) \sim x^T \beta$ -  so don't search over all functions
    \subitem this is a model based approach
    \subitem theoretical solutions is:
    $$ \beta = \left[ E(XX^T)\right]^{-1} E(XY) $$
  \item Using L1 instead of L2 as loss function leads to $ \hat{f}(x) = median(Y| X = x)$
  \item For categorical output, loss function is matrix $K\times K$ matrix $L$, where $K$ is number of categories. Zero in the diagonals, and nonnegative elsewhere
    \subitem typical is {\bf zero-one loss function} - where all off diagonals are $1$.
  \item The $EPE = E[L(G, \hat{G}(x)]$ where expectation is taken over $Pr(G, x)$
    \item using same conditioning argumetn and pointwise minimization, and zero one loss, leads to solution known as the {\bf Bayes classifier}
      $$ \hat{G} = g_k \quad \mbox{ if } \quad Pr(g_k | X = x) = max_{g \in G} Pr(g | X = x) $$
      that is, classify as the most probable class, using discrete conditional distribution $Pr(G|X)$.
      \subitem Error rate of the bayes classifier is often called the {\bf Bayes rate}.
\end{itemize}

\section{Local Methods in High Dimension}
\begin{itemize}
  \item Fitting/prediciton methods which rely on local approximations (like KNN), struggle as dimensions get high - {\bf the curse of dimentionality}
  \item example: consider p-dimensional unit hypercube with uniformly distributed inputs. If we place a sub hypercube about the origin, such that a fraction $r$ of the data is contained there,
    the linear dimension of this hypercube must be $r^{1/p}$. For 10 dimensions, the expected edge length for $r=0.01$ is $0.63$.
    \subitem so to capture one percent of the data, you must go $\%63$ of the distance avaialble in each dimension! That is not local.
  \item another manifestation comes from looking at unit sphere with uniform distribution - median value for closest to origin is about 1/2 radius. So every point close to edge.
    \subitem this is a problem because for points on the edge, prediction is often extrapolation instead of interpolation. Which is much shakier.
  \item another manifestation of this curse is that the sampling density is proportional to $N^{1/}$ where $N$ is sample size and $p$ is dimension. So if in 1-D $N=100$ represents a dense sample size, the equivalent density in 10 D is $100^{10}$. 
    \subitem so in high D, all feasable samples are sparse.
  \item {\bf Bias-variance tradeoff} - nice little ``proof'' - setup: fix a point in domain $x_0$, get a large \emph{set of training samples} $\tau$ - study the expected error in prediction at $x_0$ that comes from the sampling.
    \subitem notation: $\hat{y}_0$ is prediction using some model. $y_0$ is true value.
    \begin{eqnarray}
      MSE(x_0) &=& E_\tau[(y_0 - \hat{y}_0)^2]\\
      	&=& E_\tau[(y_0 - E_\tau[\hat{y}_0] + E_\tau[\hat{y}_0] - \hat{y}_0)^2 ] \\
	&=& {\color{orange} E_\tau[(y_0 - E_\tau[\hat{y}_0])^2]} + 2E_\tau[(y_0 - E_\tau[\hat{y}_0])(E_\tau[\hat{y}_0] - \hat{y}_0)] +   {\color{blue} E_\tau[(\hat{y}_0 - E_\tau[\hat{y}_0])^2] }
      \end{eqnarray}
      The term in blue is the variance of the prediction.
      The term in orange: $E_\tau[(y_0 - E_\tau[\hat{y}_0])^2] = (y_0 - E_\tau[\hat{y}_0])^2$ which is just the Bias of prediction squared.
      The middle term (in black) is zero (can factor out first piece, and distribute expetation over subtraction). So
      \begin{equation}
      MSE(x_0) =  {\color{orange} Bias_\tau(\hat{y}_0)^2} + {\color{blue} Var_\tau(\hat{y}_0)}
    	\end{equation}
	This relationship ends up being very generic. More on this at the end of the chapter.
\end{itemize}


\section{Statistical Models, Supervised Learning and Functional Approximation}
\begin{itemize}
  \item goal is to fint approximation $\hat{f}$ to the function $f$ that undeliers the predictive relationship between inputs and outputs
  \item Sum of squared errors loss leads to optimal $f$ being $f(x) = E(Y | X=x)$ - called the {\bf regression function}.
  \item The class of nearest neighbor methods can be viewed as a direct approximation to this conidtional expectation
  \item More generally, we seek to approximate conditional probability $Pr(Y | X)$
  \item one common model: {\bf additive error model}
    \subitem assumes:
    $$ Y = f(X) + \epsilon$$
    where $f$ is the regression function, $\epsilon$ is a random error independent of $X$ with some distribution such $E(\epsilon) = 0$
    \subitem {\red in what sense is error dependent or independent of $Y$?} - clearly for a given $x$, error has perfect correlation to $Y$ - excess $y$ is the error. But what about in general? Is that even a sensible question to ask?
    \subitem {\gray McElreath would say this is terrible way to think about it - better to just say something like $ Y \sim N(\mu_x, \sigma)$ and $\mu_x = f(x)$ - this generalized better to non additive models. The additive nature in this case evident from normal distro.}
  \item MAIN POINT: This is a specific claim about the conditional probability distribution $Pr(Y | X)$, namely that $Y$ is distributed like $\epsilon$ plus a value determined by $X$.
    \subitem moreover, note that $X$ only comes in through the conditional mean $f(x)$, and it does not come into the variance of $Y$ (though that can and is often relaxed)
  \item For quantitative responses, this is typically not the assumption, but rather that of some bernoulli process (for binary var) with $p$ of outcomes determined by $X$, that is $p(X)$. This binds both the conditional expectation and the variance to $x$.
    \subitem {\gray McElreath would say $Y \sim Bern(p_x)$ and $p_x = P(X)$.}
  \item His claim is that there are two main ways to think of this endeavor to find a good approximation for $f$
    \begin{itemize}
      \item Supervised learning - there is some algorithm that can take an input $x_i$ and map it to an output $\hat{f}(x_i)$, which can also adjust $\hat{f}$ based on the difference between predicted value and observed $y_i$. This algorithm should produce a map that can be used for predictions.
      \item Function Approximation - ${x_i, y_i}$ are viewed as points in a (p+1)-dimensional Euclidean space. The idea is that the data satisfies some relationship $ y_i = f(x_i) + \epsilon_i$, and goal is to obtain a useful approximation to $f$ thatn is valid for all points in some region.
	\subitem This paradigm encourages mathematical concepts of geometry and probability, so they prefer it.
    \end{itemize}
  \item often the approximations are restricted to some parameterized fammily of functions, and the challenge is to find the best parameter
    \subitem ex: linear model - $f(x) = x^T\beta$ (params are $\beta$), or more generally a {\bf linear basis expansion}
    $$f_\theta(x) = \sum_{k=1}^{K} h_k(x)\theta_k $$
    where $h_k$ are a suitable set of functions or transformatiosn of the input vector.
  \item often fit by minimizing the residual sum-of-squares (this is just least squares error).
  \item A more general criteria is {\bf maximum likelihood estimation} aka MLE	
    \begin{itemize}
      \item given random sample $y_i, i=1\cdots N$ from a density $Pr_\theta(y)$,
      \item log-probability of data is 
	$$ L(\theta) = \sum_i log(Pr_\theta(y_i)) $$
      \item the pricniple of MLE says most reasonable $\theta$ is that which maximizes $L(\theta)$
      \item Least squares with additive error model $Y = f_\theta(x) + \epsilon$ with $\epsilon \sim N(0, \sigma^2)$ is equivalent to MLE using conditional likelihood $Pr(Y | X) = N(f_\theta(X), \sigma^2)$
      \item Another example: Assume multinomial qualitative output $G$, with regression function $Pr(G|X)$. Suppose we have model $Pr(G = g_k | X = x) = p_{k,\theta}(x)$, then the loglikehood is
	$$ L(\theta) = \sum_{i=1}^N \log p_{g_i, \theta}(x_i) $$
	which is also referred to as the {\bf cross-entropy}
    \end{itemize}
  \item {\gray From entropy, $\int_x p_x log(p_x)$ to cross entropy $\int_x u_x log p_x$, then to observed cross entropy (the $\int_x u_x$ become the sum ovber observed values) $\sum_i log p(x_i)$}
\end{itemize}	

\section{Structured Regression Models}
\begin{itemize}
  \item infinite many solutions to min $RSS$ (they just have to interpolate between data somehow)
  \item must impose restrictions on family of potential solutions - they are controlling the \emph{complexity} of solutions in one way or another
    \subitem often impose some regularity on small neighborhoods
    \subitem size of neighborhood dicatates strength of complexity reduction (in k-NN, k controls that)
  \item main point: any method that constraints local variation in small isotropic neighborhoods will suffer curse of dimensionality; any method that overcomes the curse has some way of measuring neighborhoods which does not allow them to be small in all directions.
\end{itemize}

\section{Classes of Restricted Estimators}
\begin{itemize}
  \item main approaches listed below:
  \item Roughness penalty
    $$ PRSS(f;\lambda) = RSS(f) + \lambda J(f) $$
    where $J$ is some functional that will large for rapidly changing functions over small reguins.
    \subitem one example $J(f) = \int [f''(x)]^2 ds$. $\lambda = \infty$ only allows linear functions.
    \subitem these are also called {\bf regularization} methods
  \item Kernel methods
    \subitem estimate the regression function by specyfing nature of local neighborhoods
    \subitem use a {\bf kernel function} $K_\lambda(x_0, x)$ which assigns weights to points $x$ in a region around $x_0$
    \subitem example, Gaussian density function
    $$ K_\lambda(x_0, x) = \frac{1}{\lambda} \exp\left(-\frac{|| x-x_0||^2 }{2\lambda}\right) $$
    \subitem one example is the Nadaraya-Watson weighted average, where $\hat{f}(x)$ is the weighted sum of all $y_i$ observations times kernels $K_\lambda(x,x_i)$.
    \subitem In general we can define a \emph{local regression estimate} of $f(x_0)$ as $f_{\hat{\theta}}(x_0)$, where $\hat{\theta}$ minimizes
    $$ RSS(f_\theta, x_0) = \sum_{i=1}^N K_\lambda(x_0, x_1)(y_i - f_\theta(x_i))^2 $$
    and $f_\theta$ is some parameterized function, like a low order poly
    \subitem ex:  $f_\theta(x) = \theta_0$, results in Ndaraya-Watson
    \subitem or linear $f_\theta(x) = \theta_0 + \theta_1 x$ --- results in popular {\bf local linear regression model}
    \subitem {\gray notice that $RSS$ here depends on both $f_\theta$ and $x_0$}
  \item Basis functions 
    \subitem includes linear and polynomial expansions (and much more)
    \subitem postulate a structure
    $$ f_\theta(x) = \sum_{m=1}^M \theta_m h_m(x) $$
    \subitem note that it is linear in the $\theta$s.
\end{itemize}

\section{Model Selection and the Bias Variance tradeoff}
\begin{itemize}
  \item all of the models above has a {\bf smoothing} or {\bf complexity parameter}
  \item cannot use RSS on training data to determine these parameters
    \subitem such method ends up picking a solution that interpolates between data and hence has zero residuals, but is wild in between and not good at predicting future data.
  \item he goes through another - weirdly artifical - bias variance decomposition that show the dependency of the piece on the complexity parameter (for knn)
  \item main idea: as model complexity increases, squared bias decreases but variance increases.
  \item ideally, chose complexity parameter that leads to minimum test error.
  \item obvious estimate of test error is train error. 
  \item unfortunately test error does not properly account for error that comes from model complexity.
\end{itemize}

\section{Excercises}
\begin{itemize}
  \item 2.1. In the context of classification problem, suppose each of the $K$ classes as associated with a vector $t_k$ whose components $(t_k)_i = \delta_{k,i}$. Moreover the prediction $\hat{y}$ is a vector of probabilities (with components $y_j$), which sum to $1$. Show that classifying via choosing the biggest probability $y_g$ is equivalent to choosing the $k$ via $min_k || t_k - \hat{y} ||$.
    \subitem SOLUTION: assume L2 metric. Then must show $\sum_i (\delta_{g,i} - y_i)^2$ is less than $\sum_i (\delta_{j,i} - y_i)$ for all $j \neq g$. Just expand sum, cancel terms and it boils down to $y_g > y_j$.
  \item 2.2 - Each class has a probability distribution that is the sum of 10 normals. The bayes clasifier is all the set of points where these two distributions are equal.
  \item 2.3 - derive equation 2.24: consider N data points uniformly distributed in a p-dimensional sphere centered at the origin. Show that the median distance from the origin to the closes data point is given by
    $$ d(p, N) = \left(1 - \frac{1}{2}^{1/N}\right)^{1/p} $$
    \subitem SOLUTION: first, recall radial probaility density if $pr^{p-1}$, so CDF is $F_R(r) := P(R \leq r) = r^p$. Since he asks about the distribution of the min of $N$ points, we are dealing with order statistics. In this case we want the probability that the min value is less than or equal to some $r_0$. 
    The probability that the min is less than $r_0$ is equal to the probability that all points are greater than $r_0$.
    For a single point, the probability is $1-r_0^p$. For N points it is $(1-r_0^p)^N$. 
    So the probability that the min is less then $r_0$ is equal to $1$ minus that expression. To get the median, simply set the result equal to 1/2 and then solve for $r_0$. QED.
  \item 2.4. Another example of edge effect.
    \begin{itemize}
      \item start with spherical multinormal: $X \sim N(0, I_p)$.  distribution breaks up into product of $p$ independent $N(0,1)$.
      \item expected distance from origin is just $E(x_1^2 + \cdots +x_n^2)$ (this is distribute according to $\chi_p^2$ by definition).
      \item by linearity of expectation and the fact that $x_i$ is $N(0,1)$, the epected distance is just $p$ (each one is 1).
      \item now fix a point $x_0$ (expected ditance is $p$). and define $\hat{a} = x_0/|x_0|$ (that is unit vector in that direction)
      \item for each new sample $x_i$, let $z_i:= \hat{a} \cdot x_i$.
      \item by expanding the sum, can show hat $E(z_i) = 0$ (taking the $x_0$ values as fixed constants)
      \item similarly
	\begin{eqnarray}
	  E(z_i^2) &=&\sum_{j=1}^p E\left( \frac{(x_0)_j}{|x_0|} (x_i)_j\right)^2 + 0 \\
	   &=& \sum\frac{(x_0)_j^2}{|x_0|^2} = 1
	\end{eqnarray}
	in line one we already expanded square of sum, and dropped all terms linear in the coordinates, since they lead to zero expectations.
      \item This tells you that given a point $x_0$, its expected distance from origin if $p$. All other data points projection onto that direction have expected distance $1$. So as $p$ grows, each point sees itself as lying at the edge of the training data. 
    \end{itemize}
\end{itemize}


\chapter{Linear Methods for Regression}
\setcounter{section}{-1}
\section{LRS Summary of Main Ideas}
\begin{itemize}
  \item To understand the different perspectives, I think it is necessary to start from a very meta probability space. If we observe variables $X$ and $Y$ N times, we really have the randome meta-vector $(X_1, Y_1, \cdots X_N, Y_N)$, and this has some joint probability distribution.
  \item from this we need certain assumptions to simplify to a single $(X,Y)$ probability distribution
    \subitem presumably some spacetime-translation invariance or something like that.
  \item but at a moments notice we must be ready to switch between then single/abstract $(X,Y)$ perspective, and the each one is its own random variable perspective
  \item indeed if we fix the $X$'s (and assume independence of $Y$'s), it is useful to talk about $N$ conditional $Y|_{X=x_i}$
\end{itemize}

Now on to the main ideas in chapter
\begin{itemize}
  \item 
\end{itemize}

\setcounter{section}{1}
\section{Linear Regression Models and Least Squares}
\begin{itemize}
  \item Start with a random vector $X^T = (X_1, \cdots X_p)$ and random variable $Y$.
  \item linear model assumes that the regression function $Y = f(X)$ where $f(x) = E(Y|X=x)$ is (at least approximately) of the form  $f(x) = \sum_{j=1}^p \beta_j X_j$
    \subitem the $\beta_j$'s are uknown parameters
  \item let ${\bf X}$ be the $n\times p$ matrix of observations of $X$, and $y$ be the observations of $Y$ and $\beta = (\beta_0, \beta_1,..\beta_n)^T$
  \item minimizng squared error loss function yield the following estimate $\hat{\beta}$ 
    $$ \hat{\beta} = ({\bf X}^T {\bf X})^{-1} {\bf X}^T y $$
    \subitem minimum squared error criterion is a statistically reasonable criterion if the observations $(x_i, y_i)$ represent a random draw from their population.
    \subitem even if $x_i$'s not randomly drawn, it is still valid so long as $y_i$'s are conditionally independent given the inputs $x_i$. {\red What does this mean?}
    \subitem {\gray perhaps this means, think not of (X,Y), but rather just $Y$, whose distribution is a function of $x$, and drawing from that family of distributions. As opposed to some meta distribution where y drawings depend on previous y drawings}
  \item this process of minimizing squared errors, can be interpreted geometrically as finding the best $p$-dim hyperplane that approximates the data in $(p+1)$-dim space.
  \item fitted values at the training inputs are:
    $$ \hat{\bf y} = {\bf X}\hat{\beta} = {\bf X \left(X^T X\right)^{-1} X^T y} $$
    \subitem the matrix ${\bf H = X \left(X^T X\right)^{-1} X^T } $ is called the {\bf hat matrix} - because it puts hat on y.
  \item Hat matrix view shows a different geometrical interpretation:
    \begin{itemize}
      \item consider $\mathbb{R}^N$ this time (N number of observations)
      \item the column vectors of $X$ span a subspace of $\mathbb{R}^N$ - call it Span({\bf X})
      \item minimzing 
	$$ RSS(\beta) = || {\bf y - X}\beta||^2$$
	leads to an estimate $\hat{y}$ that lies in Span({\bf X}).
	\subitem this can be seen by actually taking a derivative wrt $\beta$, set to 0, and solving - leads to ${\bf X}^T({\bf y-X}\beta) = 0$
      \item hence ${\bf{\hat{y}}}$ is the \emph{orthgonal projection} of y, and {\bf H} is the projection matrix
	  \subitem can check it satisfies $HH=H$
	\item rank deficiencies leads to non-uniqueness. Typically taken care of by dropping some features.
    \end{itemize}
  \item now ``In order to pin down the sampling properties of $\hat{\beta}$, we now assume that the observations $y_i$ are uncorrelated and have constant variance $\sigma^2$, and that the [observations] $x_i$ are fixed (non-random)''
    \subitem {\gray I read this: consider each $y_i$ as a random variable. That is don't marginalize $y$ but rather consider conditional distribution $y = f_{Y|X=x_i}(y)$. Assume this distribution has variance $\sigma^2$. And that these random variables are pairwise uncorrelated.}
\end{itemize}

\begin{itemize}
  \item first, from solution for ${\hat{\beta}}$ we can prove that
    $$ VAR(\hat{\beta})  = ({\bf X}^T{\bf X})^{-1} \sigma^2 $$
  	\subitem This is relatively straight forward. Can prove $VAR(AZ) = A VAR(Z) A^T$ for constant matrix $A$ and random vector $Z$.
	\subitem and uncorrelated, constant variance implies $VAR(Y) = {\bf I_n} \sigma^2$.
   \item second: the typical estimate of $\sigma^2$ is
     $$ \hat{\sigma}^2 = \frac{1}{N - p - 1} \sum_{i=1}^N (y_i - \hat{y}_i)^2. $$
     This estimator is unbiased - that is: $E(\hat{\sigma}^2) = \sigma^2$.
   \item basic idea of proof: \underline{Not in ESL}
     \begin{itemize}
       \item recall, ${\bf X}$ values are fixed (thus so is ${\bf H}$). Consider random vector $y = (y_1,\cdots,y_n)$ (where each is $Y$ conditional on fixed $x_i$). Further consider the random vector
	 $$ \yh := {\bf H} y. $$
	 \subitem recall what is still random is the as-yet to be observed $y = (y_1, \cdots y_n)$, which implies the $\hat{\beta}$ are random variables, and thus $\yh$ are too.
       \item now
	 $$  y -  \yh =  (1 - {\bf H})y $$
	 substituting $y = {\bf X}\beta + \epsilon$ and using $(1-{\bf H)X} = 0$ yields
	 $$ y - \yh = (1 - {\bf H})\epsilon $$
	 \subitem {\gray note that $y-X\beta = \epsilon$, but $y - \yh = (1-H)\epsilon$, that is due to the difference in $\beta$ and $\hat{\beta}$.}
       \item $ 1 -{\bf H}$ is also a projection matrix, and it projects onto space perpendicular so Span($X$). Call that space Span($X$)${}^\perp$.
       \item there exits an $N\times (N-p-1)$ matrix $U$ such that $1 - {\bf H}$ can be written as $UU^T$ (by nature of being a projection operator)
	 \subitem basically $U$ has as columns an orthnormal basis for the space Span$(X)^\perp$ (which is (N-p-1) dimensional)
       \item so we have
	 \begin{eqnarray}
	   \sum_{i=1}^N (y_i - \yh_i)^2 &=& || y -\yh ||^2  \\
	   	&=& || UU^T\epsilon ||^2 \\
		&=& \epsilon^TUU^TUU^T \epsilon \\
		&=& \epsilon^T UU^T \epsilon \\
	      	&=& || U^T\epsilon ||^2	
	 \end{eqnarray}
       \item Now, the assumption is that $\epsilon = (\epsilon_1, \cdots \epsilon_N) \sim N(0, I_N\sigma^2)$, so $U^T\epsilon \sim N(0, U^TU\sigma^2)$ and $U^TU = I_{N-p-1}$
       \item so $E||U^T\epsilon||^2 = (N - p - 1) \sigma^2 $
       \item more specifically $||U^T\epsilon|| \sim \sigma^2\chi^2_{N-p-1}$
       \item comment: clearly, for this proof we already added the more strict assumption referred to in the next lines
     \end{itemize}
   \item Then ``To draw inferences about the parameters and the model, additional assumptions are needed'' - asssume:
     \begin{itemize}
       \item the conditional expectation of $Y$ really is linear in the $X$
       \item the deviations of $Y$ around its expectation are additive and gaussian, hence
	 \begin{equation}
	   Y = E(Y|X_1, \cdots X_p) + \epsilon 
	 \end{equation}
	 where $\epsilon \sim N(0, \sigma^2)$
     \end{itemize}
     Now it is easy to show 
     $$ \hat{\beta} \sim N(\beta, ({\bf X}^T {\bf X})^{-1} \sigma^2) $$
     \subitem to do this, think of each observation $y_i$ as a random variable $Y_i$, which by assumption is $Y_i = E(Y|X=x_i) + \epsilon_i$, where the $\epsilon_i$ are i.i.d $N(0,\sigma^2)$. Plugging this in, yields said result. 
   \item in summary
     \begin{eqnarray}
	\hat{\beta} &\sim& N(\beta, ({\bf X}^T {\bf X})^{-1} \sigma^2)  \\
	\hat{\sigma^2} &\sim& \frac{\sigma^2}{N-p-1} \chi^2_{N-p-1}
     \end{eqnarray}
   \item moreover they are statistically independent. {\red (How do we know this? they seem very tied together?)}
   \item now we create hypothesis tests and confidence intervals
     \begin{itemize}
       \item for a specific coefficient $\beta_j$ we the {\emph standardized coefficient} or {\bf Z-score} is defined as
	 $$ z_j := \frac{\hat{\beta}_j}{\hat{\sigma} \sqrt{v_j}} $$
       where $v_j$ is the j-th diagonal element of $({\bf X^T X})^{-1}$
     \item first note $\hat{\beta}_j \sim N(\beta_j, \sigma^2 v_j)$ - this is just marginalizing the multivariate distribution. so $\hat{\beta}_j/\sqrt{v_j} \sim N(\beta_j, \sigma^2)$.
     \item  so $z_j$ has a ration distribution between a normal and a chi distribution. Using the ration of distributions theorem, and a lot math, you can show that $z_j$ has a student-t distribution with $N - p -1$ dof, assuming that $\beta_j = 0$ (the null hypothesis).
       \subitem hence large absolute values of $z_j$ lead to rejection of the null hypothesis.
     \item if testing whether a group of coefficients have an impact together, we use the {\bf F-statistic}
       $$ F = \frac{(RSS_0 - RSS_1)/(p_1-p_0)}{RSS_1/(N-p_1 - 1)}  $$
       where subscript $1$ is for the bigger model with $p_1 + 1$ parameters, and $0$ is for the nested smaller model, having $p_1 - p_0$ parameters constrained to be zero.
       \subitem the normalization factor is an estimate of $\sigma^2$.
   \end{itemize}
 \end{itemize}

\subsection{Gauss Markov}
 \begin{itemize}
 \item {\blue Gauss Markov theorem}
   \begin{itemize}
     \item VIP result!
     \item context: assume linear model $ y = X\beta + \epsilon$
     \item assume we are trying to estimate any linear combination of the $\beta$ parameters - say $\theta = a^T\beta$
       \subitem one such example is prediction $f(x_0) = x_0^T\beta$
     \item claim: least squares estimate 
       $$ \hat{\theta} = a^T \hat{\beta} = a^T {\bf (X^TX)^{-1}X^T}y $$
       has the smallest variance among all \emph{unbiased}, \emph{linear estimates}.
       \subitem this is linear in the following sense: considering ${\bf X}$ to be fixed, then $\hat{\theta}$ is a linear function $c_0^Ty$ of the response vector $y$.
     \item in other words, given any other estimate $\tilde{\theta} = c^Ty$, such that $E(c^Ty) = a^T\beta$, then
       $$ Var(a^T\beta) \leq Var(c^Ty) $$
     \item Proof of this and slight generalization is excercise 3.3
   \end{itemize}
 \item Why do we care?
   \begin{itemize}
     \item MSE for an estimator $\tilde{\theta}$, estimating $\theta$, can be decomposed (bias-var tradeoff)
       $$ MSE(\tilde{\theta}) = E(\tilde(\theta) - \theta)^2 = Var(\tilde{\theta}) + \left(E(\tilde{\theta} - \theta\right)^2 $$
       \item so amongst all unbiased estimators, the lowest error is the lowest variance
       \item moreover, expected prediction error, which is what we ultimately care about, is intimately tied to MSE
	 \subitem given prediction $Y_0 = f(x_0) + \epsilon_0$ for new input $x_0$, the EPE for an estimate $\tilde{f}(x_0)$ is
	 $$ E(Y_0 - \tilde{f}(x_0)) = \sigma^2 + MSE(\tilde{f}(x_0)) $$
   \end{itemize}
 \item However, if we lift unbiased restriction, we can often get an estimator with even lower MSE. Will see next section.
   \subitem All models are wrong (here read biased), but some models are useful.
\end{itemize}

\subsection{From Univariate to MultiVariate regression}
\begin{itemize}
  \item start with no-intercept, univarate linear regression 
    $$ Y = X\beta + \epsilon$$
  \item letting ${\bf y} = (y_1,\cdots y_N)^T$ and ${\bf x} = (x_1, \cdots x_N)^T$, and using $\langle a, b \rangle$ to denote inner prod, then we can write
    \begin{eqnarray}
      \hat{\beta} &=& \frac{\langle \bx, \by \rangle}{\langle \bx, \bx \rangle} \\
      {\bf r} &=&  \by - \bx \hat{\beta}
    \end{eqnarray}
    where ${\bf r}$ is the resisual vector
  \item Now, consider the \emph{multiple linear regression model} where inputs are all orthgonal - that is $\langle \bx_i, \bx_j \rangle = 0$ for $i\neq j$
    \subitem this means that $\bX^T\bX$ is diagonal with values $\langle \bx_i, \bx_i \rangle$.
    \subitem this leads to 
    $$ \hat{\beta}_j = \frac{\langle \bx_j, \by \rangle}{\langle \bx_j, \bx_j \rangle} $$
    which is just the regression coefficient for each one independently
  \item So the difference between man univariate regression coeffieicents and a general multivariate regression solution, is the extent to which the input variables are not orthgonal to each other.
    \subitem  {\gray NOTE THEY ARE NOT SAYING UNCORRELATED - BUT ORTHGONAL!}
  \item now consider \emph{ univariate regression} with an intercept. Can show that the least squares coefficient of $\bx$ has the form
    $$ \frac{\langle\bx - \bar{x}{\bf 1}, \by \rangle}{\langle \bx - \bar{x}{\bf 1}, \bx - \bar{x}{\bf 1}\rangle} $$
	\subitem this can be arrived at by 1) regressing $\bx$ on ${\bf 1}$ and getting the residual ${\bf z} = \bx - \bar{x}{\bf 1}$ and then 2) reressing $\by$ on residual ${\bf x}$
      \item  This can be generalized to $p$ variables, into what is called {\bf Regression by Succesive orthgonalization} aka {\bf Gram-Schmidt procedure for multiple regression}
   \item process: choose a variable whose coeffient you will calulate, say $\bx_k$, then
     \begin{itemize}
       \item initialize ${\bf z}_0 = \bx_0 = 1$
       \item for all $j \neq k$ regress $x_j$ on all the ${\bf z}$s produce so far. They are orthogonal, so coefficient is univariate regression coefficient.
	 \subitem then get the residual ${\bf z}_j$. 
       \item finally regress $\bx_k$ on all the $p-1$ ${\bf z}$s, and obtain its residual ${\bf z}_k$.
	 \begin{equation}
	   \hat{\beta}_p = \frac{\langle {\bf z}_k, \by \rangle}{\langle {\bf z}_k, {\bf z}_k \rangle} \label{eq:coeff1}
	 \end{equation}
       \item Then regress $\by$ on ${\bf z}_k$ to give ${\hat{\beta}}_k$.
     \end{itemize}
   \item repeat this, from scratch for each $p$
   \item since all the ${\bf z}$s are orthogonal, they form a basis space for Span \bX, therefore the least squares projection onto this space is ${\yh}$. Since ${\bf z}_k$ alone involves $\bx_k$, with coefficient 1, that means that ${\bf z}_k$ coefficient of $\yh$ is indeed the $\bx_k$ coefficient.
   \item in conclusion: ``The multiple regression coefficient $\hat{\beta}_j$ represents the additional contribution of $\bx_j$ on $\by$, after $\bx_j$ has been adjusted for $\bx_0, \cdots, \bx_{j-1}, \bx_{j+1},\cdots,\bx_p$.
   \item from (\ref{eq:coeff1}) can obtain $Var(\hat{\beta}_k) = \sigma^2/||{\bf z}_k||$.
     \subitem this says that the precision with which we can estimate that coefficient depends on the length of the residual - that is, how much new info is in that 
   \item One pass with {\bf QR decomposition} via Gram schmidt
     \begin{itemize}
       \item after one pass of gram-schmidt, can write
	 $$ \bX = {\bf Z}{\bf \Gamma} $$
	where ${\bf Z}$ has as columns the ${\bf z}_j$ (in order), and the ${\bf \Gamma}$ is the upper triangular matrix that converts it to $\bX$ - basically the regression coefficients obtained during gram-schmidt
      \item Introduce the diagonal matrix ${\bf D}$, with jth diagonal entry $D_jj = ||{\bf z}_j||$
	\begin{eqnarray}
	  \bX &=& {\bf Z D^{-1} D \Gamma } \\
	   &=& {\bf QR}
	\end{eqnarray}
	this is the \emph{QR decomposition} of \bX. ${\bf Q}$ satisfies ${\bf Q}^T{\bf Q} = {\bf I}$ and ${\bf R}$ is upper triangular.
      \item can show that
	\begin{eqnarray}
	  {\bf R}\hat{\beta}  &=& {\bf Q}^T\by \\
	  \hat{\by} &=& {\bf QQ}^{T}\by
	\end{eqnarray}
	The top equation is easy to solve because $R$ is upper triangular.

     \end{itemize}
   \item can easily generalize to multiple outputs.
     \subitem Basically write each component as its own regression
     \subitem if erros uncorrelated, just sum all dimensions - least square solution has exactly same form
     \subitem if there is reason to believe errors are correlated, then loss function is different, but optimal estimate is still the same.
\end{itemize}

\section{Subset selection}
\begin{itemize}
  \item Main idea: use training data to produce sequence of models of varying complexity, indexed by a single parameter,
    \subitem Use CV or AIC (will talk about later) to choose value of complexity parameter
    \subitem CV is performed using only training data since it is being used to train the metaparameter.
  \item four approaches using choice of predictors
    \begin{itemize}
      \item best-subset selection
	\subitem for each $N \in \{ 0, \cdots, p\}$, choose best choice of predictors.
      \subitem feasible only for $p \sim\leq 40$
      \item forward step-wise selection
	\subitem start with just intercept, then one at a time add predictor that most improves fit
	\subitem greedy algorithm
	\subitem much faster computationally and much more constrained, so less variance but more bias
      \item backwards stepwise selection
	\subitem similar but start with full set and take one out at a time (drop var with smallest z-score)
      \item forward stagewise selection
	\subitem start with $ y = \yb + 0x_1 + \cdots $ with $x$s centered
	\subitem compute residual; pick var most correlated to residual; add regression to residual
	\subitem this is slow, but good in high dimensions
	\subitem eventually converges to OLS
    \end{itemize}
  \item be careful - must account for multiple testing problem of overfitting
  \item also must consider variables that naturally come in groups (like dummy variables)
  \item {\red question: in the ten-fold CV procedure, say in best subset selection, for a given number of predictors $k$, each fold might yield a different set of predictors. So in choosing the best model, we cannot really be choosing the best $k$ predictors, but rather choosing the parameter $k$. Correct? And then fit it again using the whole training data?}
  \item The CV procedure can be used to provide standard error bands
  \item in the end they use the  {\bf one standard error rule} - pick the most parsimonious model within one standard error of the minimum.
\end{itemize}

\section{Shrinkage Methods}
\begin{itemize}
  \item Ridge regression - impose L2 penalty on coefficients
    \begin{equation}
      \hat{\beta}_{ridge} = argmin_{\beta} \left\{ \sum_{i=1}^N\left(y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j  \right)^2 + \lambda \sum_{j=1}^p \beta_j^2\right\} \label{eq:ridge}
    \end{equation}
    \subitem here $\lambda$ id s complexity parameter
    \subitem there is an equivalent formulation in using regular OLS with constraint that sum of squares of betas are less than some fixed value (which is tunable - and related to $\lambda$ above).
  \item VIP: notice that the intercept $\beta_0$ has been left out of the penalty term. Including it would make procedure dependent on origin of $Y$.
  \item VIP: \underline{The ridge solutions are not equivalent under scaling of the inputs}. So one normally standardizes inputs before solving.
  \item this procedure is very useful when there are many correlated variables
  \item Note: if we start with centered $X$ - namely use $\tilde{x}_{ij} := x_{ij} - \bar{x}_j$  - and obtain ridge regression coefficients $\hat{\beta}_c$, simple substitution into ridge definition (\ref{eq:ridge}), yields that centered and uncentered solutions are related via
    \begin{eqnarray}
      \beta^0 &=& \beta_c^0 + \sum_{j=1}^p \bar{x}_j\beta_c^j \label{eq:3.5} \\
      \beta^j &=& \beta_c^j
    \end{eqnarray}
  \item Furthermore, the $\beta_0$ appears only in the first piece, and its minimum value can be easily shown to be (in the centerd case)
    $$ \beta_0 = \bar{y} $$
  \item The remaining problem is now reduced to a regression without intercept, with centered $X$ and $Y$. Henceforth we can assume that this is the starting point of ridge regression.
    \begin{itemize}
      \item we can rewrite ridge criterion as
	$$ RSS(\lambda) = (\by - \bX \beta)^T(\by - \bX \beta) $$
      \item solution is
	$$ \hat{\beta}^{ridge} = \left( \bX^T\bX + \lambda I\right)^{-1}\bX^T\by $$
    \end{itemize}
  \item Note solution is again linear in $\by$.
  \item The addition of $\lambda I$ to Gram matrix makes it non-singular. This was the original motivation for introduction of ridge regression.
  \item ridge regression can also be seen as the mean or mode of the bayesian posterior with likelihood $ y_i \sim N(\beta_0 + x_i^T \beta, \sigma^2)$ and priors $\beta_j \sim N(0, \tau^2)$, with both $\tau$ and $\sigma$ assumed known.
  \item can get more color on this via an SVD of the centered $X$ matrix (which is now $N\times p$)
    \begin{itemize}
      \item Let  $X = UDV^T$
      \item in regular OLS
	\begin{eqnarray}
	  \bX\hat{\beta}^{ls}&=& \bX(\bX^T\bX)^{-1}\bX^T\by  \\
	    &=& UU^T\by
	  \end{eqnarray}
	  Note that $U^T\by$ are the coordinates of $\by$ wrt to orthonormal basis $U$. 
	  \subitem {\gray Proof of this requires the use of the inverse of $V$, which I think exists by construction (even if $X$ and/or $X^TX$ are not full rank. Furthermore, just in writing expression, we are using $(X^TX)^{-1}$}
      \item with ridge solutions
	\begin{eqnarray}
	  X\hat{\beta}^{ridge}&=& X(X^TX + \lambda I)^{-1}X^TY  \\
	    &=& UD(D^2 + \lambda I)^{-1} DU^Ty \\
	    &=& \sum_{j=1}^p u_j \frac{d^2_j}{d^2_j + \lambda}u_j^T y
	  \end{eqnarray}
	  where the $u_j$ are the columns of $U$.
	\item So in both cases, it first computes the coordinates of $y$ in the $U$ basis. But in the case of ridge, it then shrinks these coordinates by the middle factor.
	\item moreover, note that geater shrinkage is applied to coordinates with smaller $d_j^2$.
	\item Since SVD of center matrix is another way to express PCA, this is telling us that more shrinkage is applied to the orthogonal directions of least variance.
	  \subitem ``Ridge regression protects against potentially high variance of gradients estimted in the short directions. The implicit assumption is that the response will tend to vary most in the direction of high variance of the inputs. This is often a reasonable assumption, since predictor are often chosen for study because they vary with the response variable, but need not hold in general.''
	\item can define effective degrees of freedom as
	  \begin{eqnarray}
	    df(\lambda) &=& tr\left[ X(X^TX + \lambda I)^{-1}X^T\right] \\
	      &=& tr(H_\lambda) \\
	      &=& \sum_{j=1}^p \frac{d^2_j}{d^2_j + \lambda}
	  \end{eqnarray}
	 This goes from $p$ when $\lambda = 0$ to $0$ when $\lambda \to \infty$.
    \end{itemize}
\end{itemize}

\subsection{The Lasso}
\begin{itemize}
  \item Definition
    $$\bh^{lasso} := argmin_{\beta} \sum_{i=1}^N \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2$$
    subject to
  $$ \sum_{j=1}^p |\beta_j| \leq t $$
\item as with ridge, can reparameterize $\beta_0$ by standardizing predictors; soln for $\bh_0$ is $\yb$, and thereafter can fit model without intercept {\gray (and centered y)}.
\item also like ridge, can write this in equivalent lagrangian form
  $$\bh^{lasso} := argmin_{\beta}\left\{ \frac{1}{2} \sum_{i=1}^N \left( y_i - \beta_0 - \sum_{j=1}^p x_{ij}\beta_j \right)^2 + \lambda\left( \sum_{j=1}^p |\beta_j| \right)\right\} $$
\item The L1 penalty makes solution non-linear in the $y$, and there is no closed form expression for it.
\item making $t$ sufficiently small ($\lambda$ sufficiently big) will cause some of the coefficients to be exactly zero. There are several ways to see this.
  \begin{itemize}
    \item First, note that in the case of orthonormal predictors, ridge, lasso, and subset selection have explicit solutions. Let $\bh$ be regular OLS solution, then
      \begin{center}
      \begin{tabular}{lc}
	\hline
	Estimator & Formula \\
	\hline \\
	Best susbset (size M) & $\bh \cdot I(|\bh_j| \geq |\bh_{(M)}|)$ \\
	Ridge &  $\bh_j/(1 + \lambda)$\\
	Lasso &   $sgn(\bh_j)\left(|\bh_j| - \lambda  \right)_+$\\
      \end{tabular}
    \end{center}
  \item From the non-lagrangian perspective, in 2D, the L1 constraint is a diamond centered on the origin in space of solutions $\beta$, while the L2 constraint is a circle centered at the origin. For an initial unconstrained OLS solution $(\beta_0, \beta_1)$, outside the constraint region, level surfaces of squared error formula will be ellipses centered around that original solution, with minimum error being said solution. The regularized solution will be the point on the smallest ellipse that first crosses the constraint boundary. The claim is that it is much more likely for this point to be on the corners for the diamond, than on similar points for the circle. Moreover this phenomena becomes more poignant in higher dimensions.
  \item {\gray a third way of seeing this, which is really just an extension of the first, write L1 and L2 problems down in a simple 2D case, with everything already centered. To avoid proliferation of subscripts, let predictors be $(u,v)$ and coeffs $(\beta, \gamma)$:
      $$ f(\beta, \gamma) = \sum_{i}\left( y_i - (\beta u_i + \gamma v_i)  \right) - \lambda g(\beta, \gamma) $$
      where $g$ is $\beta^2 + \gamma^2$ for ridge and $|\beta| + |\gamma|$ for lasso (actually for simplicity lasso has a factor of 1/2 our front. Will assume its there and appropriately remove it below)
      \subitem minimize and set to zero leads to
      \begin{eqnarray}
	-y\cdot u + \beta u^2 + \gamma u\cdot v + \lambda \frac{\partial g}{\partial \beta} &=& 0 \\
	-y\cdot u + \beta u\cdot v + \gamma u^2 + \lambda \frac{\partial g}{\partial \gamma} &=& 0 
	\end{eqnarray}
	\subitem for simplicity assume $u, v$ are orthonormal. Then you have
      \begin{eqnarray}
	-y\cdot u + \beta + \lambda \frac{\partial g}{\partial \beta} &=& 0 \\
	-y\cdot u + \gamma + \lambda \frac{\partial g}{\partial \gamma} &=& 0 
	\end{eqnarray}
	\subitem for ridge, $\partial g/\partial \beta = 2\beta$, so the first equation becomes
	$$ \beta = y\cdot u /(1 + \lambda)$$
	\subitem on the other hand, for lasso $\partial g /\partial \beta = 2\theta(\beta) - 1$, where $\theta$ is the heaviside step function, and the first equation becomes
	$$ \beta = (y\cdot u - \lambda (2\theta(\beta) - 1)) $$
	(I am being cavalier about factors of two for succinctness, though admittedly perhaps at the sacrifice of too much clarity)
	\subitem So, lasso puts the lambda in the numerator, therefore big enough lambda, but finite, should be able to get it to zero. With some analysis, you can see that it doesnt take you past zero. OTOH ridge puts lambda in the denominator, which will only make beta go to zero as $\lambda \to \infty$

    }
  \end{itemize}
\item can try other values of $q$ for $L_q$, but not very profitable. $q=1$ is special in its ability to set some coefficients to zero
\item better compromise is to use an elastic net penalty
  $$ \lambda\sum_{j=1}^p \left(\alpha \beta_j^2 + (1-\alpha)|\beta_j|\right) $$
\end{itemize}

\subsection{Least Angle Regression}
\begin{itemize}
  \item
\end{itemize}


\section{Excercises}

excercise 3.1
\begin{itemize}
  \item Uses known relationship where student-t squared is f-distribution (for the appropriate parameters)
\end{itemize}

excercise 3.3
\begin{itemize}
  \item The typical proof for this is all over the web - it starts by letting $c = a + d$, deriving a constraint on $d$ based on the unbiased nature and the using that constraint in tha variance expansion. The generalization to vector/matrix is straightforwadr (part b).
  \item I attempted a slitghly different proof. Here is the gist:
  \begin{itemize}
    \item OLS estimator $\theta = a \cdot \hat{\beta}$ where $a$ is a constant and $[a] = p\times 1$.
    \item other linear estimator $\tilde{\theta} = c \cdot y$ which is also unbiased
      \subitem thus $c \cdot \bar{y} = a \cdot \beta$
    \item cauchy schwarz says
      $$ c\cdot c \geq \frac{(c \cdot \bar{y})^2}{\bar{y}\cdot \bar{y}} $$
    \item now $c\cdot\bar{y} = a \cdot \beta$ so 
      \begin{eqnarray}
	(c.\bar{y})^2 &=& (a \cdot \beta)^2 \\
			&=& (a \cdot E(\hat{\beta})^2 \\
			  &=& (a \cdot E\left((X^TX)^{-1}X^Ty\right))^2 \\
			  &=& (a \cdot (X^TX)^{-1}X^T\yb)^2 
      \end{eqnarray}
    \item now the last line can be rewritten as
      \begin{eqnarray}
	\cdots &=& (a^T(X^TX)^{-1}X^T\yb)(\yb^TX(X^TX)^{-1}{}^Ta^T)
      \end{eqnarray}
    \item now, I don't see how to do it right now, but if we could group the middle $\yb$ together and somehow factor it out, then the expression would simplify to 
      $$a^T(X^TX)^{-1}a(\yb^T\yb).$$
    \item then put back into cauchy-schwarz above would yield
      $$c\cdot c \geq a^T(X^TX)^{-1}a$$
      which is what was to be proven. but I must justify that leap somehow. Perhaps not a strict equality, but using an inequality. Will continue down this path later.
  \end{itemize}
\end{itemize}

excercise 3.4
\begin{itemize}
  \item see companion notebook: Ch3\_02\_GramSchmidtRegressionAndQRDecomp
\end{itemize}

excercise 3.5
\begin{itemize}
  \item Done in situ, see equation (\ref{eq:3.5}).
\end{itemize}

excercise 3.6
\begin{itemize}
  \item Very straighforward. Just multiply normals and take negative log.
\end{itemize}


\end{document}
